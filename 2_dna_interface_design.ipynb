{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a walk-through for DBP design in Glasscock. et al., 2023. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,json\n",
    "sys.path.append('/projects/protein-DNA-binders/dna_binder_manuscript/code_repo/dbp_design/software/')\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas\n",
    "from optparse import OptionParser\n",
    "import time\n",
    "import glob\n",
    "import design_utils\n",
    "import useful_utils\n",
    "import seaborn as sns\n",
    "import string\n",
    "import random\n",
    "import pyrosetta\n",
    "from pyrosetta import *\n",
    "from pyrosetta.rosetta import *\n",
    "import pyrosetta.distributed.io as io\n",
    "import pyrosetta.distributed.packed_pose as packed_pose\n",
    "import pyrosetta.distributed.tasks.rosetta_scripts as rosetta_scripts\n",
    "import pyrosetta.distributed.tasks.score as score\n",
    "sys.path.append('/projects/protein-DNA-binders/scripts/silent_tools')\n",
    "import silent_tools\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import subprocess\n",
    "import math\n",
    "import jupyter_utils\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import itertools\n",
    "import sklearn\n",
    "import re\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn import neural_network\n",
    "from sklearn import svm\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import confusion_matrix,roc_curve,auc\n",
    "from collections import defaultdict\n",
    "from scipy.optimize import curve_fit\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rosetta_db = '/software/rosetta/latest/database'\n",
    "rosetta_scripts = '/software/rosetta/latest/bin/rosetta_scripts.hdf5.linuxgccrelease'\n",
    "     \n",
    "# USER INPUT\n",
    "ID = 'CGCACCGACTCACG'\n",
    "user= 'cjg263'\n",
    "rifdock_path = f'/net/scratch/{user}/de_novo_dna/results/rifdock_stage/rifdock_stage_SEP2022/'\n",
    "mpnn1_path = f'/net/scratch/{user}/de_novo_dna/results/mpnn_design_stage_1/mpnn1_{ID}_SEP2022/'\n",
    "inpainting1_path = f'/net/scratch/{user}/de_novo_dna/results/inpainting/inpainting1_{ID}_SEP2022/'\n",
    "mpnn2_path = f'/net/scratch/{user}/de_novo_dna/results/mpnn_design_stage_2/mpnn2_{ID}_SEP2022/'\n",
    "mpnn3_path = f'/net/scratch/{user}/de_novo_dna/results/mpnn_design_stage_3/mpnn3_{ID}_SEP2022/'\n",
    "af2_path = f'/net/scratch/{user}/de_novo_dna/results/af2_superposition/af2_{ID}_SEP2022/'\n",
    "\n",
    "os.makedirs(mpnn1_path, exist_ok=True)\n",
    "os.makedirs(inpainting1_path, exist_ok=True)\n",
    "os.makedirs(mpnn2_path, exist_ok=True)\n",
    "os.makedirs(mpnn3_path, exist_ok=True)\n",
    "os.makedirs(af2_path, exist_ok=True)\n",
    "\n",
    "# Setup various directories\n",
    "cmds_dir = 'cmds_design'\n",
    "\n",
    "#shutil.rmtree(cmds_dir)\n",
    "os.makedirs(cmds_dir, exist_ok=True)\n",
    "\n",
    "ligandmpnn_apptainer = '/software/containers/users/robpec/PPI_design_mpnn.sif'\n",
    "ligandmpnn_script = '/projects/protein-DNA-binders/dna_binder_manuscript/code_repo/dbp_design/2b_design_mpnn/mpnn_design.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## MPNN Stage 1 Prefilter Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pilot/predictor run is first used to calibrate prefilters for MPNN stage 1. Follow the steps below to run the pilot on a subset of silent files from RIFdock. The commands below sample 5 RIFdock outputs from each silent file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get silent files\n",
    "input_silents = glob.glob(f'{rifdock_path}/{ID}_rifdock_output/*.silent')\n",
    "print(len(input_silents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_silents_rand = random.sample(input_silents,10000)\n",
    "cmds = f'{mpnn1_path}/cmds_design_prefilter'\n",
    "os.makedirs(f'{mpnn1_path}/prefilter/',exist_ok=True)\n",
    "with open(cmds,'w') as f_out:\n",
    "    n_commands=0\n",
    "    for silent in input_silents_rand:   \n",
    "        silent_name = silent.split('/')[-1].replace(\".silent\",\"\")\n",
    "        output_dir = f'{mpnn1_path}/prefilter/{silent_name}/'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        # Build cmd\n",
    "        cmd = f'cd {output_dir}; \\\n",
    "                {ligandmpnn_apptainer} {ligandmpnn_script} -silent {silent} \\\n",
    "                                -seq_per_struct 1 \\\n",
    "                                -n_per_silent 5 \\\n",
    "                                -temperature 0.1 \\\n",
    "                                -freeze_hbond_resis 0 \\\n",
    "                                -bb_phos_cutoff 1 \\\n",
    "                                -hbond_energy_cut -0.5 \\\n",
    "                                -run_predictor 1 \\\n",
    "                                -run_relax 1\\n'\n",
    "        f_out.write(cmd)\n",
    "        \n",
    "env = 'source activate /home/nrbennet/miniconda3/envs/ampere'\n",
    "if os.path.exists('logs'): shutil.rmtree('logs')\n",
    "jupyter_utils.make_submit_file(cmds=cmds, env=env, submitfile=cmds+'.sh', group_size=20, queue='cpu-bf', cpus=1, mem='6G')\n",
    "#p = subprocess.Popen(['sbatch', cmds+'.sh'])\n",
    "n_cmds = sum([1 for l in open(cmds,'r')])\n",
    "print(f'Prepared and submitted {n_cmds} jobs for clustering in {cmds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the ligand specific dataframes\n",
    "prefilter_dir = f'prefilter_calibration'\n",
    "os.makedirs(prefilter_dir, exist_ok=True)   \n",
    "\n",
    "csv_fs = glob.glob(f'{mpnn1_path}/prefilter/*/*.csv')\n",
    "df = pd.concat([pd.read_csv(f) for f in csv_fs], sort=False)\n",
    "\n",
    "df.to_csv(f'{mpnn1_path}/prefilter.csv')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{mpnn1_path}/prefilter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ddg_over_cms'] = df['ddg'] / df['contact_molecular_surface']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description'] = df['tag']\n",
    "predictor_df = df[df['is_prefilter']]\n",
    "pilot_df = df[~df['is_prefilter']]\n",
    "\n",
    "predictor_df = predictor_df[predictor_df['ddg'] <= 0]\n",
    "print(len(predictor_df))\n",
    "pilot_df = pilot_df[pilot_df['ddg'] <= 0]\n",
    "print(len(pilot_df))\n",
    "\n",
    "predictor_df = predictor_df[predictor_df['contact_molecular_surface'] > 10]\n",
    "print(len(predictor_df))\n",
    "pilot_df = pilot_df[pilot_df['contact_molecular_surface'] > 10]\n",
    "print(len(pilot_df))\n",
    "\n",
    "\n",
    "def suffix_all_columns(df, suffix):\n",
    "    cols = list(df.columns)\n",
    "    for i in range(len(cols)):\n",
    "        cols[i] = cols[i] + suffix\n",
    "    df.columns = cols \n",
    "    \n",
    "suffix_all_columns(predictor_df, \"_pred\")\n",
    "predictor_df['tag'] = predictor_df['tag_pred']\n",
    "\n",
    "pilot_df = pilot_df.merge(predictor_df, 'inner', 'tag')\n",
    "#predictor_df['description'] = predictor_df['outpdb']\n",
    "#pilot_df['description'] = pilot_df['outpdb']\n",
    "\n",
    "print(\"Length of predictor dataframe: \", len(predictor_df))\n",
    "print(\"Length of pilot dataframe: \", len(pilot_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The format here is:\n",
    "# Name of the score in pilot, cut value, higher better, name in predictor, is integer score-term\n",
    "\n",
    "terms_and_cuts = {\n",
    "    'ddg_over_cms':                      [ -0.076,False, \"ddg_over_cms_pred\", False],\n",
    "    'contact_molecular_surface':[225,   True, \"contact_molecular_surface_pred\", False],\n",
    "}\n",
    "\n",
    "pilot_df = pilot_df.dropna(subset=terms_and_cuts.keys())\n",
    "predictor_df = predictor_df.dropna(subset=[terms_and_cuts[x][2] for x in terms_and_cuts])\n",
    "\n",
    "\n",
    "score_df = pilot_df\n",
    "\n",
    "# Filter all the terms and print the thresholds\n",
    "ok_terms = []\n",
    "for pilot_term in terms_and_cuts:\n",
    "    cut, good_high, term, is_int = terms_and_cuts[pilot_term]\n",
    "    ok_term = pilot_term.replace(\"_pilot\", \"\") + \"_ok\"\n",
    "    if ( good_high ):\n",
    "        score_df[ok_term] = score_df[pilot_term] >= cut\n",
    "    else:\n",
    "        score_df[ok_term] = score_df[pilot_term] <= cut\n",
    "    \n",
    "    ok_terms.append(ok_term)\n",
    "    \n",
    "    print(\"%30s: %6.2f\"%(pilot_term, cut))\n",
    "\n",
    "# Print the pass rates for each term\n",
    "print(\"\")\n",
    "score_df['orderable'] = True\n",
    "for ok_term in ok_terms:\n",
    "    score_df['orderable'] &= score_df[ok_term]\n",
    "    print(\"%30s: %5.0f%% pass-rate\"%(ok_term.replace(\"_ok\", \"\"), score_df[ok_term].sum() / len(score_df) * 100))\n",
    "\n",
    "# print the overall pass rate   \n",
    "subdf = score_df\n",
    "print(\"Orderable: %i   -- %.2f%%\"%(subdf['orderable'].sum(), (100*subdf['orderable'].sum() / len(subdf))))\n",
    "\n",
    "# Plot and compare to natives\n",
    "relevant_features  = terms_and_cuts.keys()\n",
    "ncols = 3\n",
    "nrows = math.ceil(len(relevant_features) / ncols)\n",
    "(fig, axs) = plt.subplots(\n",
    "    ncols=ncols, nrows=nrows, figsize=[15,6*nrows]\n",
    ")\n",
    "axs = axs.reshape(-1)\n",
    "\n",
    "for (i, metric) in enumerate(terms_and_cuts):\n",
    "    pred_m = terms_and_cuts[metric][2]\n",
    "#     is_int = terms_and_cuts[metric][3]\n",
    "#     n_ints = len(set(list(pilot_df[metric])+list(pilot_df[pred_m])))\n",
    "#     n_bins = n_ints if is_int else None\n",
    "    sns.distplot(pilot_df[metric], ax=axs[i], color='blue', label='pilot')\n",
    "    sns.distplot(predictor_df[pred_m], ax=axs[i], color='orange', label='predictor')\n",
    "\n",
    "    if i==0:\n",
    "        axs[i].legend()\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a few and check filters:\n",
    "df_test = subdf[subdf['orderable']==True]\n",
    "with open(f'{mpnn1_path}/tags.list','w') as f:\n",
    "    for j,row in df_test.iterrows():\n",
    "        tag = row['tag']\n",
    "        f.write(tag+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Maximum Likelihood functions\n",
    "\n",
    "eqs = []\n",
    "\n",
    "def prob_of_1term(value, zdf, pilot_term, terms_and_cuts):\n",
    "    cut, good_high, term, is_int = terms_and_cuts[pilot_term]\n",
    "    \n",
    "    cut_div = cut + 0.01\n",
    "    if (is_int):\n",
    "        cut_div = 1\n",
    "#     print(value)\n",
    "    representatives = zdf[abs( (zdf[term] - value) / cut_div ) < 0.02]\n",
    "    \n",
    "    if ( good_high ):\n",
    "        fail = (representatives[pilot_term] < cut).sum()\n",
    "        ok = (representatives[pilot_term] >= cut).sum()\n",
    "    else:\n",
    "        fail = (representatives[pilot_term] > cut).sum()\n",
    "        ok = (representatives[pilot_term] <= cut).sum()\n",
    "        \n",
    "    if (fail + ok < 5):\n",
    "        return np.nan, 1\n",
    "    return ok / (fail + ok), fail + ok\n",
    "\n",
    "\n",
    "def get_low_high_bin_size_low_bin_num_bins(dfz, pilot_term, terms_and_cuts):\n",
    "    cut, good_high, term, is_int = terms_and_cuts[pilot_term]\n",
    "    cut_div = cut + 0.01\n",
    "    \n",
    "    low = dfz[term].min()\n",
    "    high = dfz[term].max()\n",
    "    if (abs( (low - cut) / cut_div ) > 1000 ):\n",
    "        print(\"Crazy value!!!\", pilot_term, low, high)\n",
    "        assert(False)\n",
    "        \n",
    "    bin_size = abs(cut_div * 0.02)\n",
    "    \n",
    "    if ( is_int ):\n",
    "        bin_size = 1\n",
    "    low_bin = math.floor( low / bin_size )\n",
    "\n",
    "    num_bins = math.floor( high / bin_size ) - low_bin + 1\n",
    "    \n",
    "    return low, high, bin_size, low_bin, num_bins\n",
    "    \n",
    "# Find the index of xs that maximumizes the following equation\n",
    "# np.sum( xs[:i] * flip < divisor ) + np.sum( xs[i:] * flip > divisor )\n",
    "def find_ind_that_divides( divisor, array, flip ):\n",
    "    best_sum = 0\n",
    "    best_i = None\n",
    "    for i in range(len(array)):\n",
    "        value = np.sum( array[:i] * flip < flip * divisor ) + np.sum( array[i:] * flip > flip * divisor )\n",
    "        if ( value > best_sum ):\n",
    "            best_sum = value\n",
    "            best_i = i\n",
    "    return best_i\n",
    "    \n",
    "def sigmoid(x, a, b):\n",
    "    return 1 / ( 1 + np.exp( -( x - a) * b ) )\n",
    "\n",
    "def smooth_empty_prob_array(arr, good_high, counts, graphs=False, low=0, bin_size=0, gd=None, term=\"\"):\n",
    "    counts = list(counts)\n",
    "    x = list(range(0, len(arr)))\n",
    "    to_remove = []\n",
    "    for i in range(len(arr)):\n",
    "        if (math.isnan(arr[i])):\n",
    "            to_remove.append(i)\n",
    "    arr_copy = list(arr)\n",
    "    for i in reversed(to_remove):\n",
    "        x.pop(i)\n",
    "        arr_copy.pop(i)\n",
    "        counts.pop(i)\n",
    "    arr_copy = np.array(arr_copy)\n",
    "\n",
    "#     print(good_high)\n",
    "\n",
    "    # We're trying to fit a sigmoid here. I've found that while the\n",
    "    # function will often converge with a nonsense starting point\n",
    "    # if you want it to be robust, you need to calculate the parameters\n",
    "    # by hand first\n",
    "\n",
    "    # The xguess is where the sigmoid crosses 0.5\n",
    "    \n",
    "    \n",
    "    flip = 1 if good_high else -1\n",
    "    \n",
    "    never_high = max(arr_copy) < 0.5\n",
    "    never_low = min(arr_copy) > 0.5\n",
    "    \n",
    "    # Your data is totally garbage\n",
    "    if (never_high and never_low):\n",
    "        xguess = x[int(len(x)/2)]\n",
    "        \n",
    "    # Your data is all below 0.5, assign xguess to edge\n",
    "    elif ( never_high ):\n",
    "        if ( good_high ):\n",
    "            xguess = x[-1]\n",
    "        else:\n",
    "            xguess = x[0]\n",
    "            \n",
    "    # Your data is all above 0.5, assign xguess to edge\n",
    "    elif (never_low):\n",
    "        if ( good_high ):\n",
    "            xguess = x[0]\n",
    "        else:\n",
    "            xguess = x[-1]\n",
    "            \n",
    "    else:\n",
    "        # here we have full range data\n",
    "        # pick x that maximizes the following function\n",
    "        # np.sum( arr_copy[:x] < 0.5 ) + np.sum( arr_copy[x:] > 0.5 )\n",
    "       \n",
    "        best_ix = find_ind_that_divides(0.5, arr_copy, flip)\n",
    "        xguess = x[best_ix]\n",
    "        \n",
    "            \n",
    "    # Ok, now let's work on the slope guess\n",
    "    # Formula is: guess = ln( 1 / y - 1) / ( xdist from 0.5 to y)\n",
    "    # We'll use y = 0.2 and 0.8\n",
    "    \n",
    "    never_high = max(arr_copy) < 0.8\n",
    "    never_low = min(arr_copy) > 0.2\n",
    "    \n",
    "    # Data never goes above 0.8, assign xvalue to edge\n",
    "    if ( never_high ):\n",
    "        if ( good_high ):\n",
    "            ub = x[-1]\n",
    "        else:\n",
    "            lb = x[0]\n",
    "    else:\n",
    "    # Find xvalue that corresponds to graph crossing 0.8\n",
    "        best_ix = find_ind_that_divides(0.8, arr_copy, flip)\n",
    "        if ( good_high ):\n",
    "            ub = x[best_ix]\n",
    "        else:\n",
    "            lb = x[best_ix]\n",
    "        \n",
    "    # Data never goes below 0.2, assign xvalue to edge\n",
    "    if ( never_low ):\n",
    "        if ( good_high ):\n",
    "            lb = x[0]\n",
    "        else:\n",
    "            ub = x[-1]\n",
    "    else:\n",
    "    # Find xvalue that corresponds to graph crossing 0.2\n",
    "        best_ix = find_ind_that_divides(0.2, arr_copy, flip)\n",
    "        if ( good_high ):\n",
    "            lb = x[best_ix]\n",
    "        else:\n",
    "            ub = x[best_ix]\n",
    "            \n",
    "    # One side of the data is bad, just use the other side\n",
    "    if ( ub <= xguess ):\n",
    "        ub = xguess - lb + xguess\n",
    "    if ( lb >= xguess ):\n",
    "        lb = xguess - ( ub - xguess )\n",
    "    \n",
    "    # The data is really bad here, just assign the ub and lb to the edges\n",
    "    if ( ub == lb ):\n",
    "        lb = x[0]\n",
    "        ub = x[-1]\n",
    "            \n",
    "    # Average our two distances\n",
    "    critical_dist = (( ub - xguess ) + (xguess - lb )) / 2\n",
    "    \n",
    "    # Find slope guess\n",
    "    slope_guess = np.abs( np.log( 1 / 0.2 - 1) / critical_dist ) * flip\n",
    "            \n",
    "    # Curve fit\n",
    "    popt, pcov = curve_fit( sigmoid, x, arr_copy, p0=(xguess, slope_guess), maxfev=100000, sigma=1/np.sqrt(counts) )\n",
    "    \n",
    "    # Uncomment this if you're debugging the guesses (They do really well tbh)\n",
    "#     popt = (xguess, slope_guess)\n",
    "    \n",
    "    # Our new fitted data\n",
    "    arr2 = sigmoid(np.arange(0, len(arr), 1), popt[0], popt[1])\n",
    "    \n",
    "    a_prime = popt[0]*bin_size+low\n",
    "    b_prime = popt[1]/bin_size\n",
    "    global eqs\n",
    "    eqs.append( \" 1 / ( 1 + EXP( -( %s - %.5f ) * %.5f ) ) \"%(term[:-5], a_prime, b_prime))\n",
    "    \n",
    "    \n",
    "    if (graphs):\n",
    "        plt.figure(figsize=(5,3))\n",
    "        sns.set(font_scale=1)\n",
    "        plt.plot(np.arange(0, len(arr), 1)*bin_size+low, arr)\n",
    "        plt.plot(np.arange(0, len(arr), 1)*bin_size+low, arr2)\n",
    "        if (gd):\n",
    "            plt.xlim([gd[0], gd[1]])\n",
    "            plt.xlabel(gd[2])\n",
    "            plt.axvline(gd[3], color='r')\n",
    "        plt.ylabel(\"P( passing filter )\")\n",
    "        sns.set(font_scale=1.8)\n",
    "        plt.show()\n",
    "    \n",
    "    for i in range(len(arr2)):\n",
    "        arr[i] = arr2[i]\n",
    "    \n",
    "def create_prob_array(low, high, low_bin, num_bins, bin_size, pilot_term, dfz, terms_and_cuts, graphs=False):\n",
    "    cut, good_high, term, is_int = terms_and_cuts[pilot_term]\n",
    "    arr = np.zeros(num_bins)\n",
    "    for i in range(len(arr)):\n",
    "        arr[i] = np.nan\n",
    "        \n",
    "    counts = np.zeros(num_bins)\n",
    "    counts.fill(1)\n",
    "\n",
    "    print(\"%s from %.3f to %.3f\"%(pilot_term, low, high))\n",
    "    for val in np.arange(low, high + bin_size, bin_size/2):\n",
    "        binn = math.floor( val / bin_size ) - low_bin\n",
    "        if (binn >= len(arr)):\n",
    "            continue\n",
    "        if (is_int):\n",
    "            val = round(val, 1)\n",
    "#         print(val)\n",
    "        prob, count = prob_of_1term(val, dfz, pilot_term, terms_and_cuts)\n",
    "    \n",
    "        counts[binn] = count + 1\n",
    "        if ( math.isnan(prob)):\n",
    "            pass\n",
    "        else:\n",
    "            arr[binn] = prob\n",
    "            \n",
    "    gd = None\n",
    "    try:\n",
    "        gd = graph_data[pilot_term]\n",
    "    except:\n",
    "        pass\n",
    "    smooth_empty_prob_array(arr, good_high, counts, graphs, low, bin_size, gd, term)        \n",
    "    \n",
    "    return arr\n",
    "\n",
    "def apply_prob_arrays(dfz, prob_arrays, prob_name):\n",
    "    prob_terms = []\n",
    "    for term in prob_arrays:\n",
    "        print(term)\n",
    "        arr, bin_size, low_bin = prob_arrays[term]\n",
    "        prob_term = term + \"_prob\"\n",
    "        idx = (np.floor(dfz[term] / bin_size) - low_bin).astype(\"int\")\n",
    "        is_low = (idx < 0)\n",
    "        is_high = (idx >= len(arr) ).sum()\n",
    "        low = np.min(idx)\n",
    "        high = np.max(idx)\n",
    "        \n",
    "#         if (is_low.sum() > 0 or is_high.sum() > 0):\n",
    "#             print(\"Warning: bounds overshoot on %s [%.3f, %.3f]\"%\n",
    "#                   (term, low_bin * bin_size, (low_bin + len(arr)) * bin_size))\n",
    "#             print(\"Below: %i Below_median: %.3f Below_max: %.3f Above: %i Above_median: %.3f Above_max: %.3f\"%(\n",
    "#                 is_low.sum(), (0 - np.median(idx[is_low]))*bin_size, (0 - low)*bin_size,\n",
    "#                 is_high.sum(), (np.median(idx[is_high]) - len(arr))*bin_size, (high-len(arr))*bin_size\n",
    "#             ))\n",
    "        \n",
    "        idx = np.clip(idx, 0, len(arr)-1)\n",
    "        dfz[prob_term] = arr[ idx ]\n",
    "        prob_terms.append(prob_term)\n",
    "    dfz[prob_name] = 1\n",
    "    for prob_term in prob_terms:\n",
    "        dfz[prob_name] *= dfz[prob_term]\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "def train_and_predict_mle(df, all_indices, test_indices, terms_and_cuts, prob_name, whole_df, graphs=False):\n",
    "\n",
    "    use_indices = list(set(all_indices) - set(test_indices))\n",
    "\n",
    "    test_df = df.iloc[test_indices].copy(True)\n",
    "    use_df = df.iloc[use_indices].copy(True)\n",
    "\n",
    "    \n",
    "    prob_arrays = {}\n",
    "\n",
    "    for pilot_term in terms_and_cuts:\n",
    "        cut, good_high, term, is_int = terms_and_cuts[pilot_term]\n",
    "        low, high, bin_size, low_bin, num_bins = get_low_high_bin_size_low_bin_num_bins(whole_df, pilot_term, terms_and_cuts)\n",
    "        prob_array = create_prob_array(low, high, low_bin, num_bins, bin_size, pilot_term, use_df, terms_and_cuts, graphs)\n",
    "        prob_arrays[term] = (prob_array, bin_size, low_bin)\n",
    "    \n",
    "    apply_prob_arrays(test_df, prob_arrays, prob_name)\n",
    "\n",
    "    return test_df[[prob_name, 'description']], prob_arrays\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is a little out-dated, but it still works\n",
    "# We have the test_indices set to 0.1% because the curve fitting removes memorization\n",
    "\n",
    "train_df = pilot_df.copy(True)\n",
    "all_indices = list(range(len(train_df)))\n",
    "test_indices = []\n",
    "\n",
    "# This sets up maximum likihood method\n",
    "not_used, prob_arrays = train_and_predict_mle(train_df, all_indices, test_indices, terms_and_cuts, \"predict\", predictor_df, True)\n",
    "\n",
    "print(\"\")\n",
    "print('-predictor_filters ' + \",\".join( terms_and_cuts[x][2][:-5] for x in list(terms_and_cuts)))\n",
    "print('-equation=\"-' + \"*\".join(eqs) + '\"')\n",
    "with open(f'{mpnn1_path}/filter_eq.txt','w') as f_out:\n",
    "    f_out.write('\"-' + \"*\".join(eqs) + '\"\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the mle method to the training set to get a feel for how well it worked\n",
    "apply_prob_arrays(pilot_df, prob_arrays, \"predict\")\n",
    "pilot_df['log_predict'] = np.log10(pilot_df['predict'])\n",
    "plot_df = pilot_df\n",
    "fpr,tpr,thresholds = roc_curve(plot_df[\"orderable\"], plot_df[\"predict\"])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(fpr, tpr, 'r', label = \"Predictor auc = %.2f\"%(auc(fpr, tpr)))\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Make a cool graph to get a feel for where different predict values lie\n",
    "df_c=pilot_df.sort_values(\"predict\", ascending=False)\n",
    "df_c['total_orderable'] = df_c['orderable'].cumsum()\n",
    "df_c['log_predict'] = np.log10(df_c['predict'])\n",
    "\n",
    "\n",
    "cmap = sns.cubehelix_palette(as_cmap=True)\n",
    "\n",
    "lowb = np.percentile(df_c['log_predict'], 2)\n",
    "upb = np.percentile(df_c['log_predict'], 98)\n",
    "f, ax = plt.subplots(figsize=(7, 4))\n",
    "points = ax.scatter(range(len(df_c)), df_c['total_orderable'], c=df_c['log_predict'], vmin=lowb,vmax=upb, cmap=cmap)\n",
    "plt.setp(ax.get_xticklabels(), visible=False)\n",
    "plt.setp(ax.get_yticklabels(), visible=False)\n",
    "cb = f.colorbar(points)\n",
    "cb.set_label(\"log_predict\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "apply_prob_arrays(predictor_df, prob_arrays, \"predict\")\n",
    "predictor_df['log_predict'] = np.log10(predictor_df['predict'])\n",
    "minimum = predictor_df['log_predict'].min()\n",
    "maximum = predictor_df['log_predict'].max()\n",
    "steps = 20\n",
    "step = (maximum - minimum)/steps\n",
    "probability_mapping_x = np.arange(minimum, maximum, step)\n",
    "probability_mapping_y = []\n",
    "\n",
    "last_prob = None\n",
    "for step_prob in probability_mapping_x:\n",
    "    upper = step_prob + step\n",
    "    total = pilot_df[(pilot_df['log_predict'] > step_prob) & (pilot_df['log_predict'] < upper)]\n",
    "    orderable = total['orderable'].sum()\n",
    "    if ( len(total) < 10 ):\n",
    "        prob = last_prob\n",
    "    else:\n",
    "        prob = orderable / len(total)\n",
    "    probability_mapping_y.append(prob)\n",
    "    last_prob = prob\n",
    "# fill in the beginning\n",
    "last_prob = probability_mapping_y[-1]\n",
    "for i in range(len(probability_mapping_y)):\n",
    "    i = len(probability_mapping_y) - i - 1\n",
    "    if ( probability_mapping_y[i] is None ):\n",
    "        probability_mapping_y[i] = last_prob\n",
    "    last_prob = probability_mapping_y[i]\n",
    "\n",
    "probability_mapping_y = np.array(probability_mapping_y)\n",
    "    \n",
    "plt.xlabel(\"log_predict\")\n",
    "plt.ylabel(\"Pilot success rate\")\n",
    "plt.scatter(probability_mapping_x, probability_mapping_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the mle to the predictor data and see how the values look\n",
    "\n",
    "apply_prob_arrays(predictor_df, prob_arrays, \"predict\")\n",
    "predictor_df['log_predict'] = np.log10(predictor_df['predict'])\n",
    "bounds = (np.percentile(predictor_df['log_predict'], 1), np.percentile(predictor_df['log_predict'], 99))\n",
    "sns.distplot(predictor_df['log_predict'].clip(bounds[0], bounds[1]))\n",
    "plt.title(\"All predicted data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction_to_design = 0.05\n",
    "topXp = int(len(predictor_df)*fraction_to_design)\n",
    "MLE_cut = list(sorted(-predictor_df['log_predict']))[topXp]\n",
    "print(f'To predict for the top {fraction_to_design*100}% use an MLE cutoff > {-MLE_cut}')\n",
    "print(f'In your data set this corresponds to {topXp} successes out of {len(predictor_df)}')\n",
    "\n",
    "with open(f'{mpnn1_path}/filter_cut.txt','w') as f_out:\n",
    "    f_out.write(str(-MLE_cut) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## MPNN Stage 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the steps below to run MPNN on the entire RIFdock outputs. The commands below use the prefilter calibration files generated in \"MPNN Stage 1 Prefilter Calibration\" to pre-empt full Rosetta relax on designs that do not pass the filter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get silent files\n",
    "input_silents = glob.glob(f'{rifdock_path}/{ID}_rifdock_output/*.silent')\n",
    "print(len(input_silents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell launches jobs to predict MPNN sequences from rifdock outputs. \n",
    "# By default, 1 sequence will be generated per dock. \n",
    "# Predictor is run on all outputs. The best 5% (calibration above) will be relaxed.\n",
    "\n",
    "prefilter_eq_file = f'{mpnn1_path}/filter_eq.txt'\n",
    "prefilter_mle_cut_file = f'{mpnn1_path}/filter_cut.txt'\n",
    "\n",
    "cmds = f'{mpnn1_path}/cmds_design_production'\n",
    "os.makedirs(f'{mpnn1_path}/production/',exist_ok=True)\n",
    "with open(cmds,'w') as f_out:\n",
    "    n_commands=0\n",
    "    for silent in input_silents:   \n",
    "        silent_name = silent.split('/')[-1].replace(\".silent\",\"\")\n",
    "        output_dir = f'{mpnn1_path}/production/{silent_name}/'\n",
    "        if os.path.isfile(output_dir + 'check.point'):\n",
    "            with open(output_dir + 'check.point', 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                if len(lines) > 900:\n",
    "                    continue\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        # Build cmd\n",
    "        cmd = f'cd {output_dir}; \\\n",
    "                {ligandmpnn_apptainer} {ligandmpnn_script} -silent {silent} \\\n",
    "                                -seq_per_struct 1 \\\n",
    "                                -temperature 0.1 \\\n",
    "                                -freeze_hbond_resis 0 \\\n",
    "                                -bb_phos_cutoff 1 \\\n",
    "                                -hbond_energy_cut -0.5 \\\n",
    "                                -prefilter_eq_file {prefilter_eq_file} \\\n",
    "                                -prefilter_mle_cut_file {prefilter_mle_cut_file} \\\n",
    "                                -checkpoint_path {checkpoint_path} \\\n",
    "                                -run_predictor 1 \\\n",
    "                                -run_relax 1\\n'\n",
    "        f_out.write(cmd)\n",
    "        n_commands += 1\n",
    "        \n",
    "env = 'source activate /home/nrbennet/miniconda3/envs/ampere'\n",
    "if os.path.exists('logs'): shutil.rmtree('logs')\n",
    "jupyter_utils.make_submit_file(cmds=cmds, env=env, submitfile=cmds+'.sh', group_size=round(n_commands/1000), timeout='12:00:00', queue='cpu-bf', cpus=1, mem='6G')\n",
    "p = subprocess.Popen(['sbatch', cmds+'.sh'])\n",
    "n_cmds = sum([1 for l in open(cmds,'r')])\n",
    "print(f'Prepared and submitted {n_cmds} jobs for clustering in {cmds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect results from mpnn designs\n",
    "combined_csv = 'combined.csv'\n",
    "combined_silent = 'combined.silent'\n",
    "useful_utils.collect_results(mpnn1_path,'production/*/',combined_csv,combined_silent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell filters the resulting outputs. You may play with filters to get a desired number of filtered outputs that look reasonable to you.\n",
    "# You may change fraction not passing to allow some number of designs that are below the filter thresholds. \n",
    "# This is useful for testing if filters are helping in experiments.\n",
    "# Use seqid_cut to cluster by sequence identify. seqid_cut of 1 will not cluster by sequence. \n",
    "\n",
    "fraction_not_passing = 0\n",
    "seqid_cut = 1\n",
    "\n",
    "df = pd.read_csv(f'{mpnn1_path}/combined.csv')\n",
    "df = df[df['ddg'] < 0]\n",
    "df = df[df['contact_molecular_surface'] > 0]\n",
    "\n",
    "threshold_dict = {'base_score':[10,'>='],'phosphate_score':[0, '>='],'bidentate_score':[1, '>='],\n",
    "                  'max_rboltz_RKQE':[0.15, '>='],'avg_top_two_rboltz':[0.1, '>='],'n_backbone_phosphate_contacts':[0,'>='],\n",
    "                  'ddg':[-15,'<='],'contact_molecular_surface':[225,'>='],'net_charge_over_sasa':[-10,'>='],\n",
    "                  'ddg_over_cms':[-0.06,'<='],'mpnn_score':[2,'<=']\n",
    "                    }\n",
    "\n",
    "df_filtered = useful_utils.filter_designs(mpnn1_path,df,threshold_dict,fraction_not_passing,seqid_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SILENT_FILE = f'{mpnn1_path}/combined.silent'\n",
    "\n",
    "### Save passing designs into a filtered silent.\n",
    "selected_designs = []\n",
    "for j in range(len(df_filtered)):\n",
    "    selected_designs.append(str(df_filtered['tag'].iloc[j]))\n",
    "print(len(selected_designs))\n",
    "\n",
    "with open(f'{mpnn1_path}/filtered_designs.list', 'w') as f:\n",
    "    for line in selected_designs:\n",
    "        f.write(line + '\\n')\n",
    "        \n",
    "print('Run this in the terminal to get your filtered silent file:\\n')\n",
    "print(f\"cat filtered_designs.list | silentslice {SILENT_FILE} > combined_filtered.silent\")\n",
    "\n",
    "df_filtered.to_csv(f'{mpnn1_path}/combined_filtered.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"mkdir filtered_pdbs\n",
    "cd filtered_pdbs\n",
    "silentextract ../combined_filtered.silent\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your final pwm filtered results\n",
    "\n",
    "\n",
    "SILENT_FILE = f'{mpnn1_path}/combined_filtered.silent'\n",
    "\n",
    "# These are your designs!!!\n",
    "selected_designs = []\n",
    "for j in range(len(df_pwm_filtered)):\n",
    "    selected_designs.append(str(df_pwm_filtered['tag'].iloc[j]))\n",
    "print(len(selected_designs))\n",
    "\n",
    "\n",
    "with open(f'{mpnn1_path}/pwm_filtered_designs.list', 'w') as f:\n",
    "    for line in selected_designs:\n",
    "        f.write(line + '\\n')\n",
    "        \n",
    "print('Run this in the terminal to get your filtered silent file:\\n')\n",
    "print(f\"\"\"\n",
    "cat pwm_filtered_designs.list | silentslice {SILENT_FILE} > combined_filtered_pwm.silent\"\"\")\n",
    "\n",
    "df_pwm_filtered.to_csv(f'{mpnn1_path}/combined_filtered_pwm.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Inpainting: Scaffold Loop diversification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is an optional section diversify designs with loop inpainting. This step could easily be replaced with RFdiffusion or partial RFdiffusion for loop inpainting, which is a more up to date technique. To perform design diversification by inpainting you will need to intall Inpainting from https://github.com/RosettaCommons/RFDesign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract pdbs from pwm filtered stage 2 designs\n",
    "pwm_filtered_silent = f'{mpnn1_path}/combined_filtered_pwm.silent'\n",
    "print(f\"\"\"\n",
    "cd {inpainting1_path}\n",
    "mkdir complex_pdbs\n",
    "cd complex_pdbs\n",
    "silentextract {pwm_filtered_silent}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdbs = glob.glob(f'{inpainting1_path}/complex_pdbs/*pdb')\n",
    "print(len(pdbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "init()\n",
    "cmds = f'{inpainting1_path}/cmds_inpainting'\n",
    "\n",
    "os.makedirs(f'{inpainting1_path}/input_pdbs',exist_ok=True)\n",
    "os.makedirs(f'{inpainting1_path}/output_pdbs',exist_ok=True)\n",
    "\n",
    "for pdb in pdbs:\n",
    "    pose = pose_from_pdb(pdb)\n",
    "\n",
    "    bb_phos_resis = useful_utils.get_bb_phos_contacts(pose)\n",
    "    hbond_resis = useful_utils.count_hbonds_protein_dna(pose)\n",
    "    fix_resis = bb_phos_resis + hbond_resis\n",
    "\n",
    "    fix_resis = [str(j) for j in fix_resis]\n",
    "    const_pos = \",\".join(fix_resis)\n",
    "\n",
    "    input_pdb = inpainting1_path + 'input_pdbs/' + pdb.split('/')[-1]\n",
    "    useful_utils.write_new_pdb(pdb,input_pdb)\n",
    "    \n",
    "    with open(input_pdb.replace('.pdb','.cst'),'w') as f_out:\n",
    "        f_out.write(const_pos)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{inpainting1_path}/gen_contigs.sh','w') as f_out:\n",
    "    f_out.write(f\"\"\"\n",
    "#!/bin/bash \n",
    "\n",
    "\n",
    "folder='{inpainting1_path}/input_pdbs/'\n",
    "\n",
    "# spits out os.path.join(folder, 'dssp_all.csv')\n",
    "/projects/protein-DNA-binders/dna_binder_manuscript/code_repo/dbp_design/software/get_folder_dssp.py -folder $folder \n",
    "\n",
    "dssp_path=$folder'dssp_all.csv'\n",
    "\n",
    "\n",
    "echo $dssp_path \n",
    "\n",
    "script='/projects/protein-DNA-binders/scripts/auto_contigs.py'\n",
    "# apologies for the terrible variable names \n",
    "N=2\n",
    "K=5\n",
    "L=4\n",
    "num_term=5\n",
    "M=5\n",
    "\n",
    "source  activate mlfold \n",
    "\n",
    "python $script --ss_csv $dssp_path -out ./ -K $K -N $N -l_flank $L -num_term $num_term -max_inpaint_change $M\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bash f'{inpainting1_path}/gen_contigs.sh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_depth = 20\n",
    "cmds = f'{inpainting1_path}/cmds_inpainting'\n",
    "with open(cmds,'w') as f_out:\n",
    "    with open(f'{inpainting1_path}/contigs_out.tsv','r') as f_in:\n",
    "        lines = f_in.readlines()[1:]\n",
    "        for line in lines:\n",
    "            pdb,contigs,inpaint_seq = line.rstrip().split('\\t')\n",
    "            outpdb = pdb.replace('input_pdbs','output_pdbs').replace('.pdb','')\n",
    "            cmd = f'python /home/cjg263/software/proteininpainting_autoregressive/inpaint.py \\\n",
    "                    --pdb {pdb} \\\n",
    "                    --contigs {contigs} \\\n",
    "                    --inpaint_seq {inpaint_seq} \\\n",
    "                    --out {outpdb} \\\n",
    "                    --num_designs {sampling_depth}'\n",
    "            f_out.write(cmd+'\\n')\n",
    "\n",
    "if os.path.exists('logs'): shutil.rmtree('logs')\n",
    "env = 'source activate /home/dimaio/.conda/envs/SE3nv'\n",
    "\n",
    "jupyter_utils.make_submit_file(cmds=cmds, env=env, submitfile=cmds+'.sh', timeout='15:00:00', group_size=50, queue='gpu', gres='gpu:a4000:1', cpus=2, mem='16G')\n",
    "\n",
    "p = subprocess.Popen(['sbatch', cmds+'.sh'])\n",
    "n_cmds = sum([1 for l in open(cmds,'r')])\n",
    "print(f'Prepared and submitted {n_cmds} jobs for clustering in {cmds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdbs = glob.glob(f'{inpainting1_path}/complex_pdbs/*pdb')\n",
    "print(len(pdbs))\n",
    "\n",
    "output_pdbs = glob.glob(f'{inpainting1_path}/output_pdbs/*pdb')\n",
    "print(len(output_pdbs))\n",
    "\n",
    "print(f'Expected {20*len(pdbs)} pdbs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdbs = glob.glob(f'{inpainting1_path}/complex_pdbs/*pdb')\n",
    "inpainted_pdbs = f'{inpainting1_path}/output_pdbs/'\n",
    "\n",
    "align_script = '/projects/protein-DNA-binders/dna_binder_manuscript/code_repo/dbp_design/software/inpainting_align.py'\n",
    "\n",
    "cmds = f'{inpainting1_path}/cmds_align'\n",
    "with open(cmds, 'w') as f_out:\n",
    "    for pdb in pdbs:\n",
    "        pdb_name = pdb.split('/')[-1].replace('.pdb','')\n",
    "        output_dir = f'{inpainting1_path}/align_inpaints/{pdb_name}/'\n",
    "        os.makedirs(output_dir,exist_ok=True)\n",
    "        with open(output_dir + 'pdbs.list', 'w') as f:\n",
    "            f.write(pdb)\n",
    "        cmd = f'cd {output_dir}; \\\n",
    "                {ligandmpnn_apptainer} {align_script} {inpainting1_path} {inpainted_pdbs}\\n'\n",
    "        f_out.write(cmd)\n",
    "\n",
    "if os.path.exists('logs'): shutil.rmtree('logs')\n",
    "jupyter_utils.make_submit_file(cmds=cmds, submitfile=cmds+'.sh', group_size=3, queue='cpu', timeout='1:00:00', cpus=1, mem='3G')\n",
    "#p = subprocess.Popen(['sbatch', cmds+'.sh'])\n",
    "n_cmds = sum([1 for l in open(cmds,'r')])\n",
    "print(f'Prepared and submitted {n_cmds} jobs for alignment in {cmds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get secondary structure of all inpaints for the subsequent trimming step\n",
    "pdb_folders = glob.glob(f'{inpainting1_path}/align_inpaints/*/')\n",
    "\n",
    "cmds = f'{inpainting1_path}/cmds_dssp'\n",
    "with open(cmds, 'w') as f_out:\n",
    "    for pdb_folder in pdb_folders:\n",
    "        cmd = f'/projects/protein-DNA-binders/dna_binder_manuscript/code_repo/dbp_design/software/get_folder_dssp.py -folder {pdb_folder}\\n'\n",
    "        f_out.write(cmd)\n",
    "\n",
    "if os.path.exists('logs'): shutil.rmtree('logs')\n",
    "jupyter_utils.make_submit_file(cmds=cmds, submitfile=cmds+'.sh', group_size=20, queue='cpu', timeout='1:00:00', cpus=1, mem='3G')\n",
    "#p = subprocess.Popen(['sbatch', cmds+'.sh'])\n",
    "n_cmds = sum([1 for l in open(cmds,'r')])\n",
    "print(f'Prepared and submitted {n_cmds} jobs for alignment in {cmds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Trim long sequences\n",
    "## This cell finds all pdbs with protein length > max_len, and trims around the HTH motif\n",
    "## The saved pdbs will be renumbered starting at 1\n",
    "max_len = 65\n",
    "dna_len = 28\n",
    "trim_increment = 5\n",
    "\n",
    "os.makedirs(outdir,exist_ok=True)\n",
    "\n",
    "total_trims = 0\n",
    "dssp_files = glob.glob(f'{inpainting1_path}/align_inpaints/*/dssp_all.csv')\n",
    "for dssp_file in dssp_files:    \n",
    "    complex_pdb = dssp_file.split('/')[-2]\n",
    "    with open(dssp_file,'r') as dssp_f:\n",
    "        dssp_lines = dssp_f.readlines()[2:]\n",
    "        \n",
    "    for dssp_line in dssp_lines:\n",
    "        pdb = dssp_line.split(',')[0]\n",
    "        ss = dssp_line.split(',')[1]\n",
    "        \n",
    "        ss_prot = ss[:-dna_len]\n",
    "        if len(ss_prot) <= max_len: continue\n",
    "        \n",
    "        resis, fix_resis, RH_resis, TURN_resis, AH_resis = useful_utils.get_annotation(pdb)\n",
    "        len_prot = len(resis)\n",
    "        \n",
    "        # fix_resis are the AH-TURN-RH resis, we want to add an extra SS element to these on either side of the core HTH motif.\n",
    "        # add n-terminal sec struct\n",
    "        if 'H' in ss_prot[:min(fix_resis)] or 'L' in ss_prot[:min(fix_resis)]:\n",
    "            adjacent_loop = True\n",
    "            for j in range(min(fix_resis)-1,0,-1):\n",
    "                if len(fix_resis) >= max_len: break\n",
    "                ss = ss_prot[j]\n",
    "                ss_prev = ss_prot[j+1]\n",
    "                ss_prev2 = ss_prot[j+2]\n",
    "                if ss == 'L' and adjacent_loop: \n",
    "                    fix_resis = [j] + fix_resis\n",
    "                elif ss != 'L' and ss_prev != 'L' and ss_prev2 == 'L':\n",
    "                    fix_resis = [j] + fix_resis\n",
    "                    adjacent_loop = False\n",
    "                elif ss == 'L' and not adjacent_loop:\n",
    "                    break\n",
    "                else:\n",
    "                    fix_resis = [j] + fix_resis\n",
    "            \n",
    "        # add c-terminal sec struct\n",
    "        if 'H' in ss_prot[max(fix_resis)-1:] or 'L' in ss_prot[max(fix_resis)-1:]:\n",
    "            adjacent_loop = True\n",
    "            \n",
    "            for j in range(max(fix_resis)+1,len_prot+1):\n",
    "                if j > len(ss_prot) : break\n",
    "                if len(fix_resis) >= max_len: break\n",
    "                \n",
    "                ss = ss_prot[j]\n",
    "                ss_prev = ss_prot[j-1]\n",
    "                ss_prev2 = ss_prot[j-2]\n",
    "                if ss == 'L' and adjacent_loop: \n",
    "                    fix_resis = fix_resis + [j]\n",
    "                elif ss != 'L' and ss_prev != 'L' and ss_prev2 == 'L':\n",
    "                    fix_resis = fix_resis + [j]\n",
    "                    adjacent_loop = False\n",
    "                elif ss == 'L' and not adjacent_loop:\n",
    "                    break\n",
    "                else:\n",
    "                    fix_resis = fix_resis + [j]\n",
    "\n",
    "        n = 0 \n",
    "        # generate trimmed pdbs by looping through at defined increments and ensuring all fixed resis and included in the trimmed structure\n",
    "        for j in range(1,len(ss_prot)-max_len+1,trim_increment):\n",
    "            min_resi = j\n",
    "            max_resi = j+max_len -1\n",
    "\n",
    "            if not min(fix_resis) >= min_resi or not max(fix_resis) <= max_resi: continue\n",
    "            keep_resis = [resi for resi in range(min_resi,max_resi+1)] + [dna_resi for dna_resi in range(len_prot+1, len_prot+dna_len+1)]\n",
    "\n",
    "            pdbout = pdb.replace('.pdb',f'_trim{n}.pdb')\n",
    "            with open(pdb,'r') as f_in:\n",
    "                lines = f_in.readlines()\n",
    "                with open(pdbout,'w') as f_out:\n",
    "                    for line in lines:\n",
    "                        if line.startswith('ATOM') and int(line[23:26].strip()) in keep_resis:\n",
    "                            old_resi = int(line[23:26].strip())\n",
    "                            if old_resi <= max_resi:\n",
    "                                new_resi = old_resi - (min_resi - 1)\n",
    "                            else:\n",
    "                                new_resi = old_resi - (min_resi - 1) - (len_prot - max_resi)\n",
    "                            new_line = line[:23] + format(new_resi, ' 3d') + ' ' + line[27:]\n",
    "                            f_out.write(new_line)\n",
    "                        if line.startswith('REMARK PDBinfo-LABEL:') and int(line[23:26].strip()) in keep_resis:\n",
    "                            old_resi = int(line[23:26].strip()) \n",
    "                            if old_resi <= max_resi:\n",
    "                                new_resi = old_resi - (min_resi - 1)\n",
    "                            else:\n",
    "                                new_resi = old_resi - (min_resi - 1) - (len_prot - max_resi)\n",
    "                            new_line = line[:23] + format(new_resi, ' 3d') + ' ' + line[27:]\n",
    "                            f_out.write(new_line)\n",
    "            n += 1\n",
    "            total_trims += 1\n",
    "print(total_trims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_pdbs = glob.glob(f'{inpainting1_path}/align_inpaints/*/*pdb')\n",
    "print(len(aligned_pdbs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## MPNN Stage 2: MPNN design of inpainted scaffolds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you can run loop inpainting, you will redesign the proten sequences with LigandMPNN similarly to MPNN Stage 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdbs = glob.glob(f'{inpainting1_path}/complex_pdbs/*pdb')\n",
    "print(len(pdbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell launches jobs to predict MPNN sequences from inpainting1 outputs.\n",
    "prefilter_eq_file = f'{mpnn1_path}/filter_eq.txt'\n",
    "prefilter_mle_cut_file = f'{mpnn1_path}/filter_cut.txt'\n",
    "\n",
    "cmds = f'{mpnn2_path}/cmds_design_production'\n",
    "os.makedirs(f'{mpnn2_path}/production/',exist_ok=True)\n",
    "with open(cmds,'w') as f_out:\n",
    "    n_commands=0\n",
    "    for pdb in pdbs:   \n",
    "        pdb_name = pdb.split('/')[-1].replace('.pdb','')\n",
    "        pdb_folder = f'{inpainting1_path}/align_inpaints/{pdb_name}/'\n",
    "        output_dir = f'{mpnn2_path}/production/{pdb_name}/'\n",
    "        if os.path.isfile(output_dir + 'check.point'):\n",
    "            with open(output_dir + 'check.point', 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                if len(lines) > 900:\n",
    "                    continue\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        #Build cmd\n",
    "        cmd = f'cd {output_dir}; \\\n",
    "                {ligandmpnn_apptainer} {ligandmpnn_script} -pdb_folder {pdb_folder} \\\n",
    "                                -seq_per_struct 4 \\\n",
    "                                -temperature 0.2 \\\n",
    "                                -freeze_hbond_resis 0 \\\n",
    "                                -bb_phos_cutoff 0 \\\n",
    "                                -hbond_energy_cut -0.5 \\\n",
    "                                -prefilter_eq_file {prefilter_eq_file} \\\n",
    "                                -prefilter_mle_cut_file {prefilter_mle_cut_file} \\\n",
    "                                -checkpoint_path {checkpoint_path} \\\n",
    "                                -run_predictor 1 \\\n",
    "                                -run_relax 1\\n'\n",
    "        f_out.write(cmd)\n",
    "        n_commands += 1\n",
    "        \n",
    "if os.path.exists('logs'): shutil.rmtree('logs')\n",
    "jupyter_utils.make_submit_file(cmds=cmds, env=env, submitfile=cmds+'.sh', group_size=3, timeout='12:00:00', queue='cpu-bf', cpus=1, mem='6G')\n",
    "#p = subprocess.Popen(['sbatch', cmds+'.sh'])\n",
    "n_cmds = sum([1 for l in open(cmds,'r')])\n",
    "print(f'Prepared and submitted {n_cmds} jobs for clustering in {cmds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect results\n",
    "combined_csv = 'combined_stage_2.csv'\n",
    "combined_silent = 'combined_stage_2.silent'\n",
    "useful_utils.collect_results(mpnn2_path,'production/*/',combined_csv,combined_silent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell makes a combined.silent with both MPNN stage 1 and stage 2 designs. \n",
    "mpnn1_silents = glob.glob(f'{mpnn1_path}/combined_filtered_pwm.silent')\n",
    "mpnn2_silents = glob.glob(f'{mpnn2_path}/combined_stage_2.silent')\n",
    "\n",
    "silent_out = f'{mpnn2_path}/stage_1_2_combined.silent'\n",
    "with open(silent_out, 'w') as f_out:\n",
    "    for silent in mpnn1_silents:\n",
    "        with open(silent,'r') as f_in:\n",
    "            lines = f_in.readlines()\n",
    "            for line in lines:\n",
    "                f_out.write(line)\n",
    "    for silent in mpnn2_silents:\n",
    "        with open(silent,'r') as f_in:\n",
    "            lines = f_in.readlines()\n",
    "            for line in lines:\n",
    "                f_out.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting the resulting data into a csv file. \n",
    "df_stage_1 = pd.read_csv(f'{mpnn1_path}/combined_filtered_pwm.csv')\n",
    "df_stage_1['stage'] = 1\n",
    "df_stage_1 = df_stage_1.drop(columns=['cluster'])\n",
    "print(len(df_stage_1))\n",
    "\n",
    "df_stage_2 = pd.read_csv(f'{mpnn2_path}/combined_stage_2.csv')\n",
    "df_stage_2['stage'] = 2\n",
    "df_stage_2 = df_stage_2.drop_duplicates(subset=[\"sequence\"])\n",
    "df_stage_2.to_csv(f'{mpnn2_path}/combined_stage_2.csv')\n",
    "print(len(df_stage_2))\n",
    "\n",
    "df_all = pd.concat([df_stage_1,df_stage_2])\n",
    "for column in df_all.columns:\n",
    "    if 'Unnamed' in column:\n",
    "        df_all = df_all.drop(columns=[column])\n",
    "        \n",
    "seq_lens = []\n",
    "for j, row in df_all.iterrows():\n",
    "    seq_len = len(row['sequence'])\n",
    "    seq_lens.append(seq_len)\n",
    "    \n",
    "df_all['seq_len'] = seq_lens\n",
    "df_all.to_csv(f'{mpnn2_path}/stage_1_2_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell filters the resulting outputs. You may play with filters to get a desired number of filtered outputs that look reasonable to you.\n",
    "# You may change fraction not passing to allow some number of designs that are below the filter thresholds. \n",
    "# This is useful for testing if filters are helping in experiments.\n",
    "# Use seqid_cut to cluster by sequence identify. seqid_cut of 1 will not cluster by sequence. \n",
    "\n",
    "fraction_not_passing = 0\n",
    "seqid_cut = 1\n",
    "\n",
    "df = pd.read_csv(f'{mpnn2_path}/stage_1_2_combined.csv')\n",
    "df = df[df['ddg'] < 0]\n",
    "df = df[df['contact_molecular_surface'] > 0]\n",
    "\n",
    "threshold_dict = {'base_score':[10,'>='],'phosphate_score':[0, '>='],'bidentate_score':[0, '>='],\n",
    "                  'max_rboltz_RKQE':[0.15, '>='],'avg_top_two_rboltz':[0.1, '>='],'n_backbone_phosphate_contacts':[0,'>='],\n",
    "                  'ddg':[-15,'<='],'contact_molecular_surface':[225,'>='],'net_charge_over_sasa':[-10,'>='],\n",
    "                  'ddg_over_cms':[-0.06,'<='],'mpnn_score':[2,'<='],'seq_len':[65,'<=']\n",
    "                    }\n",
    "\n",
    "df_filtered = useful_utils.filter_designs(mpnn2_path,df,threshold_dict,fraction_not_passing,seqid_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved the filtered diversified models into a new silent file.\n",
    "\n",
    "SILENT_FILE = f'{mpnn2_path}/stage_1_2_combined.silent'\n",
    "\n",
    "### Save passing designs. Stop here for continuing to Motif Graft\n",
    "selected_designs = []\n",
    "for j in range(len(df_filtered)):\n",
    "    selected_designs.append(str(df_filtered['tag'].iloc[j]))\n",
    "print(len(selected_designs))\n",
    "\n",
    "with open(f'{mpnn2_path}/filtered_designs.list', 'w') as f:\n",
    "    for line in selected_designs:\n",
    "        f.write(line + '\\n')\n",
    "        \n",
    "print('Run this in the terminal to get your filtered silent file:\\n')\n",
    "print(f\"cat filtered_designs.list | silentslice {SILENT_FILE} > stage_1_2_filtered.silent\")\n",
    "\n",
    "df_filtered.to_csv(f'{mpnn2_path}/stage_1_2_filtered.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the silent from the last step for launching MPNN diversification jobs.\n",
    "silent_file = f'{mpnn2_path}/stage_1_2_filtered.silent'\n",
    "print(f\"\"\"\n",
    "cd {mpnn2_path}\n",
    "qlogin --mem=10g\n",
    "mkdir silent_splits\n",
    "cd silent_splits\n",
    "silentsplitshuf {silent_file} 20\n",
    "cd ..\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## MPNN Stage 3: MPNN of Stage 2 FR models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an optional step. Cycles of Ligandmpnn design + Rosetta FastRelax have been shown to improve pass rates through AlphaFold2. Follow the steps below to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_silents = glob.glob(f'{mpnn2_path}/silent_splits/*.silent')\n",
    "print(len(input_silents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell launches jobs to predict MPNN sequences from rifdock outputs. \n",
    "# By default, 1 sequence will be generated per dock. \n",
    "# Predictor is run on all outputs. The best 5% (calibration above) will be relaxed.\n",
    "\n",
    "prefilter_eq_file = f'{mpnn1_path}/filter_eq.txt'\n",
    "prefilter_mle_cut_file = f'{mpnn1_path}/filter_cut.txt'\n",
    "\n",
    "cmds = f'{mpnn3_path}/cmds_design_production'\n",
    "os.makedirs(f'{mpnn3_path}/production/',exist_ok=True)\n",
    "with open(cmds,'w') as f_out:\n",
    "    n_commands=0\n",
    "    for silent in input_silents:   \n",
    "        silent_name = silent.split('/')[-1].replace(\".silent\",\"\")\n",
    "        output_dir = f'{mpnn3_path}/production/{silent_name}/'\n",
    "        \n",
    "        # Check if checkpoint file is complete. This is useful for requeuing jobs. \n",
    "        if os.path.isfile(output_dir + 'check.point'):\n",
    "            with open(output_dir + 'check.point', 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                if len(lines) == 20:\n",
    "                    continue\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        # Build cmd\n",
    "        cmd = f'cd {output_dir}; \\\n",
    "                {ligandmpnn_apptainer} {ligandmpnn_script} -silent {silent} \\\n",
    "                                -seq_per_struct 10 \\\n",
    "                                -temperature 0.1 \\\n",
    "                                -freeze_hbond_resis 0 \\\n",
    "                                -bb_phos_cutoff 0 \\\n",
    "                                -hbond_energy_cut -0.5 \\\n",
    "                                -prefilter_eq_file {prefilter_eq_file} \\\n",
    "                                -prefilter_mle_cut_file {prefilter_mle_cut_file} \\\n",
    "                                -checkpoint_path {checkpoint_path} \\\n",
    "                                -run_predictor 1 \\\n",
    "                                -run_relax 0\\n'\n",
    "        f_out.write(cmd)\n",
    "        n_commands += 1\n",
    "        \n",
    "if os.path.exists('logs'): shutil.rmtree('logs')\n",
    "jupyter_utils.make_submit_file(cmds=cmds, env=env, submitfile=cmds+'.sh', group_size=round(n_commands/1000), timeout='12:00:00', queue='cpu-bf', cpus=1, mem='6G')\n",
    "p = subprocess.Popen(['sbatch', cmds+'.sh'])\n",
    "n_cmds = sum([1 for l in open(cmds,'r')])\n",
    "print(f'Prepared and submitted {n_cmds} jobs for clustering in {cmds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect results\n",
    "combined_csv = 'combined_stage_3.csv'\n",
    "combined_silent = 'combined_stage_3.silent'\n",
    "useful_utils.collect_results(mpnn3_path,'production/*/',combined_csv,combined_silent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell makes a combined.silent with both MPNN stage 1 and stage 2 designs. \n",
    "mpnn2_silents = glob.glob(f'{mpnn2_path}/stage_1_2_filtered.silent')\n",
    "mpnn3_silents = glob.glob(f'{mpnn3_path}/combined_stage_3.silent')\n",
    "\n",
    "silent_out = f'{mpnn3_path}/stage_1_2_3_combined.silent'\n",
    "with open(silent_out, 'w') as f_out:\n",
    "    for silent in mpnn2_silents:\n",
    "        with open(silent,'r') as f_in:\n",
    "            lines = f_in.readlines()\n",
    "            for line in lines:\n",
    "                f_out.write(line)\n",
    "    for silent in mpnn3_silents:\n",
    "        with open(silent,'r') as f_in:\n",
    "            lines = f_in.readlines()\n",
    "            for line in lines:\n",
    "                f_out.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting the resulting data into a csv file. \n",
    "df_stage_1_2 = pd.read_csv(f'{mpnn2_path}/stage_1_2_filtered.csv')\n",
    "print(len(df_stage_1_2))\n",
    "\n",
    "df_stage_3 = pd.read_csv(f'{mpnn3_path}/combined_stage_3.csv')\n",
    "df_stage_3['stage'] = 3\n",
    "df_stage_3.to_csv(f'{mpnn3_path}/combined_stage_3.csv')\n",
    "print(len(df_stage_3))\n",
    "\n",
    "df_all = pd.concat([df_stage_1_2,df_stage_3])\n",
    "for column in df_all.columns:\n",
    "    if 'Unnamed' in column:\n",
    "        df_all = df_all.drop(columns=[column])\n",
    "        \n",
    "seq_lens = []\n",
    "for j, row in df_all.iterrows():\n",
    "    seq_len = len(row['sequence'])\n",
    "    seq_lens.append(seq_len)\n",
    "df_all['seq_len'] = seq_lens\n",
    "    \n",
    "df_all = df_all.drop_duplicates(subset=[\"sequence\"])\n",
    "df_all.to_csv(f'{mpnn3_path}/stage_1_2_3_combined.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## AF2 Superposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section runs AlphaFold2 on the protein monomer and swaps out the original design model with the newly folded AF2 model. Then, it calculates interface metrics on the new complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silent_file = f'{mpnn3_path}/stage_1_2_3_combined.silent'\n",
    "print(f\"\"\"\n",
    "qlogin -c 2 --mem=16g --gres=gpu:rtx2080:1 -p gpu\n",
    "cd {af2_path}\n",
    "GRES=\"gpu:rtx2080:1\"\n",
    "source activate cjg-pyrosetta\n",
    "/projects/protein-DNA-binders/dna_binder_manuscript/code_repo/dbp_design/software/silentsketchextractch1 {silent_file} > in.silent\n",
    "/projects/protein-DNA-binders/dna_binder_manuscript/code_repo/dbp_design/software/interfaceaf2create -prefix predict_monomers -script /projects/protein-DNA-binders/dna_binder_manuscript/code_repo/dbp_design/software/monomerAF2predict.py -p gpu-bf -silent in.silent -args \" -batch 1 -recycle 12\" -gres $GRES -conda /mnt/home/nrbennet/miniconda3/envs/ampere -structs_per_job 1000\n",
    "bash ./test_command.sh\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If test_command.sh is working properly run this:\n",
    "print(f\"\"\"\n",
    "cd {af2_path}\n",
    "bash run_submit.sh\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect silents when af2 predictions are complete\n",
    "af2_silents = glob.glob(f'{af2_path}/predict_monomers_runs/*/out.silent')\n",
    "print(len(af2_silents))\n",
    "with open(f'{af2_path}/af2_predictions.silent','w') as f_out:\n",
    "    for af2_silent in af2_silents:\n",
    "        with open(af2_silent, 'r') as f_in:\n",
    "            for line in f_in.readlines():\n",
    "                f_out.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to calculate rmsd to design. Continue to next step while this runs.\n",
    "cmd = f'cd {af2_path}; \\\n",
    "        /home/cjg263/de_novo_dna/notebooks/scripts/scaffoldRMSD.py \\\n",
    "        -in:file:silent {af2_path}/af2_predictions.silent \\\n",
    "        -parents {af2_path}/in.silent \\\n",
    "        -exact_match; \\n'\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch jobs to superposition AF2 predicted monomers onto design complexes.\n",
    "\n",
    "print(f\"\"\"\n",
    "mkdir swap\n",
    "cd swap\n",
    "\n",
    "AF2_MONOMER_SILENT=../af2_predictions.silent\n",
    "COMPLEX_SILENT={mpnn3_path}/stage_2_3_filtered.silent\n",
    "\n",
    "af2_monomer_silent=$(readlink -f $AF2_MONOMER_SILENT)\n",
    "complex_silent=$(readlink -f $COMPLEX_SILENT)\n",
    "\n",
    "/projects/protein-DNA-binders/scripts/divpyjobs -prefix binder_swap -script /projects/protein-DNA-binders/scripts/swap_binder.py -p short -mem 5 -c 1 -structs_per_job 200 -silent $complex_silent -args \" -af2_apo_binders $af2_monomer_silent -af2_binder_suffix _af2pred\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect silents when superposition jobs are complete\n",
    "af2_silents = glob.glob(f'{af2_path}/swap/binder_swap_runs/*/out.silent')\n",
    "print(len(af2_silents))\n",
    "with open(f'{af2_path}/superimpose.silent','w') as f_out:\n",
    "    for af2_silent in af2_silents:\n",
    "        with open(af2_silent, 'r') as f_in:\n",
    "            for line in f_in.readlines():\n",
    "                f_out.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run relax on superpositioned models. This runs a fixed backbone relax. \n",
    "superimpose_silent='../superimpose.silent'\n",
    "relax_xml = '/home/cjg263/de_novo_dna/notebooks/scripts/af2_superposition_fastrelaxinterface.xml' # <---this uses 47D flags. Change if necessary.\n",
    "\n",
    "print(f'mkdir relax; cd relax; /projects/protein-DNA-binders/dna_binder_manuscript/code_repo/dbp_design/software/testcreatearrayjobs \\\n",
    "            -prefix fixed_bb_interface_relax \\\n",
    "            -xml {relax_xml} \\\n",
    "            -flags /projects/protein-DNA-binders/dna_binder_manuscript/code_repo/dbp_design/2b_design_mpnn/flags_and_weights/RM8B_flags \\\n",
    "            -silent {superimpose_silent} \\\n",
    "            -structs_per_job 15 \\\n",
    "            -mem 6 -p cpu \\\n",
    "            -rosetta /home/rpmchugh/software/Rosetta/main/source/bin/rosetta_scripts.default.linuxgccrelease \\\n",
    "            -hard_split \\\n",
    "            ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get silent files\n",
    "input_silents = glob.glob(f'{af2_path}/relax/fixed_bb_interface_relax_runs/*/out.silent')\n",
    "print(len(input_silents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute scores for superpositioned relaxed models. \n",
    "\n",
    "flags_file = '/projects/protein-DNA-binders/dna_binder_manuscript/code_repo/dbp_design/2b_design_mpnn/flags_and_weights/flags_RM8B'\n",
    "weights_file='/projects/protein-DNA-binders/dna_binder_manuscript/code_repo/dbp_design/2b_design_mpnn/flags_and_weights/RM8B_torsional.wts'\n",
    "\n",
    "script = '/projects/protein-DNA-binders/dna_binder_manuscript/code_repo/dbp_design/software/compute_hbond_scores_rboltz_no_align.py'\n",
    "\n",
    "\n",
    "def id_generator(size=12, chars=string.ascii_uppercase):\n",
    "    return ''.join(random.choice(chars) for _ in range(size))\n",
    "\n",
    "cmds = f'{af2_path}/cmds_compute_scores'\n",
    "with open(cmds,'w') as f_out:\n",
    "    n_commands=0\n",
    "    for silent in input_silents:   \n",
    "        silent_name = silent.split('/')[-2]\n",
    "\n",
    "        # Build cmd\n",
    "        rand_string = id_generator()\n",
    "        outsilent = f'{af2_path}/compute_scores/{silent_name}/out.silent'  \n",
    "        cmd = f'{pyros} {script} --silent_in {silent} \\\n",
    "                             --silent_out {outsilent} \\\n",
    "                             --weights {weights_file} \\\n",
    "                             --hbond_energy_cut -0.5 \\\n",
    "                             --flags_file {flags_file} '\n",
    "        f_out.write(cmd+'\\n')\n",
    "        n_commands += 1\n",
    "\n",
    "if os.path.exists('logs'): shutil.rmtree('logs')\n",
    "jupyter_utils.make_submit_file(cmds=cmds, submitfile=cmds+'.sh', group_size=1, queue='short', cpus=1, mem='3G')\n",
    "p = subprocess.Popen(['sbatch', cmds+'.sh'])\n",
    "n_cmds = sum([1 for l in open(cmds,'r')])\n",
    "print(f'Prepared and submitted {n_cmds} jobs for clustering in {cmds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect superpositioned and relaxed silents.\n",
    "silent_outputs = glob.glob(f'{af2_path}/compute_scores/*/out.silent')\n",
    "print(len(silent_outputs))\n",
    "with open(f'{af2_path}/combined.silent', 'w') as outfile:\n",
    "    for silent in silent_outputs:\n",
    "        with open(silent) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect plddt data\n",
    "sc_fs = glob.glob(f'{af2_path}/predict_monomers_runs/*/out.sc')\n",
    "df_plddt = pd.concat([pd.read_csv(sc_f,sep='\\s+') for sc_f in sc_fs],sort=False)\n",
    "tags = []\n",
    "for j in range(len(df_plddt)):\n",
    "    tag = re.sub('_af2pred$','_0001_',df_plddt['description'].iloc[j],)\n",
    "    tags.append(tag)\n",
    "df_plddt['out_tag'] = tags\n",
    "df_plddt = df_plddt.drop(columns=['SCORE:','time','description'])\n",
    "print(f'{len(df_plddt)} plddt values')\n",
    "\n",
    "# Collect data for rmsd to design\n",
    "with open(f'{af2_path}/pair_rmsd.sc','r') as f_in:\n",
    "    with open(f'{af2_path}/pair_rmsd_clean.sc', 'w') as f_out:\n",
    "        lines = f_in.readlines()\n",
    "        f_out.write(lines[0])\n",
    "        for line in lines[1:]:\n",
    "            if 'all_rmsd' not in line:\n",
    "                f_out.write(line)\n",
    "df_rmsd = pd.read_csv(f'{af2_path}/pair_rmsd_clean.sc',sep='\\s+')\n",
    "tags = []\n",
    "for j in range(len(df_rmsd)):\n",
    "    tag = re.sub('_af2pred$','_0001_',df_rmsd['description'].iloc[j],)\n",
    "    tags.append(tag)\n",
    "df_rmsd['out_tag'] = tags\n",
    "print(f'{len(df_rmsd)} rmsd values')\n",
    "\n",
    "# collected computed metrics for superpositioned relaxed complexes.\n",
    "csv_fs = glob.glob(f'{af2_path}/compute_scores/*/out.csv')\n",
    "df_all = pd.concat([pd.read_csv(f) for f in csv_fs], sort=False)\n",
    "print(f'{len(df_all)} analyzed complexes')\n",
    "\n",
    "df_all = df_all.merge(df_rmsd,on='out_tag',how='inner')\n",
    "df_all = df_all.merge(df_plddt,on='out_tag',how='inner')\n",
    "df_all.to_csv(f'{af2_path}/combined.csv')\n",
    "\n",
    "print(f'{len(df_all)} superpositioned complexes')\n",
    "\n",
    "# Filter to designs with ddg < 0. Makes plots look a little prettier.\n",
    "df_designs = df_all[df_all['ddg'] < 0]\n",
    "df_designs = df_designs[df_designs['ddg'] > -150]\n",
    "print(f'{len(df_designs)} had ddg < 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell filters the resulting outputs. You may play with filters to get a desired number of filtered outputs that look reasonable to you.\n",
    "# You may change fraction not passing to allow some number of designs that are below the filter thresholds. \n",
    "# This is useful for testing if filters are helping in experiments.\n",
    "# Use seqid_cut to cluster by sequence identify. seqid_cut of 1 will not cluster by sequence. \n",
    "\n",
    "fraction_not_passing = 0\n",
    "seqid_cut = 1\n",
    "\n",
    "df = pd.read_csv(f'{af2_path}/combined.csv')\n",
    "df = df[df['ddg'] < 0]\n",
    "df = df[df['ddg'] > -150]\n",
    "df = df[df['contact_molecular_surface'] > 0]\n",
    "df = df[df['max_rboltz_RKQE'] <= 1]\n",
    "\n",
    "threshold_dict = {'base_score':[10,'>='],'phosphate_score':[0, '>='],'bidentate_score':[1, '>='],\n",
    "                  'max_rboltz_RKQE':[0.1, '>='],'avg_top_two_rboltz':[0.1, '>='],'n_backbone_phosphate_contacts':[0,'>='],\n",
    "                  'ddg':[-15,'<='],'contact_molecular_surface':[0,'>='],'net_charge_over_sasa':[-10,'>=']\n",
    "                    }\n",
    "\n",
    "df_filtered = useful_utils.filter_designs(mpnn2_path,df,threshold_dict,fraction_not_passing,seqid_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SILENT_FILE = f'{af2_path}/combined.silent'\n",
    "\n",
    "### Save passing designs.\n",
    "selected_designs = []\n",
    "for j in range(len(df_filtered)):\n",
    "    selected_designs.append(str(df_filtered['out_tag'].iloc[j]))\n",
    "print(len(selected_designs))\n",
    "\n",
    "\n",
    "with open(f'{af2_path}/filtered_designs.list', 'w') as f:\n",
    "    for line in selected_designs:\n",
    "        f.write(line + '\\n')\n",
    "        \n",
    "print('Run this in the terminal to get your filtered silent file:\\n')\n",
    "print(f\"cat filtered_designs.list | silentslice {SILENT_FILE} > combined_filtered.silent\")\n",
    "\n",
    "df_filtered.to_csv(f'{af2_path}/combined_filtered.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"mkdir filtered_pdbs\n",
    "cd filtered_pdbs\n",
    "silentextract ../combined_filtered.silent\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_pdbs = glob.glob(f'{af2_path}/filtered_pdbs/*pdb')\n",
    "print(len(filtered_pdbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launches jobs that predict PWMs for each design. \n",
    "\n",
    "filtered_pdbs = glob.glob(f'{af2_path}/filtered_pdbs/*pdb')\n",
    "cmds = f'{af2_path}/cmds_pwm'\n",
    "results_dir = f'{af2_path}/filtered_pdbs/pwm_results/'\n",
    "os.makedirs(results_dir,exist_ok=True)\n",
    "os.makedirs(f'{af2_path}/pwm_logs/',exist_ok=True)\n",
    "n=0\n",
    "with open(cmds, 'w') as f:\n",
    "    for pdb in filtered_pdbs:\n",
    "        tag = pdb.split('/')[-1].replace('.pdb','')\n",
    "        output_dir = f'{af2_path}/filtered_pdbs/pwm_results/{tag}/'\n",
    "        output_file = f'{output_dir}/specific_pos_count.txt'\n",
    "        if os.path.isfile(output_file): continue\n",
    "        cmd = f'rm -r {output_dir}; \\\n",
    "                python /projects/protein-DNA-binders/scripts/pwm_from_ddg/pwm_from_ddg.py -i {pdb} -o {results_dir} -l 10 -s 2\\n'\n",
    "        f.write(cmd)\n",
    "        n+=1\n",
    "\n",
    "if os.path.exists('logs'): shutil.rmtree('logs')\n",
    "env = '/home/cjg263/.conda/envs/cjg-pyrosetta'\n",
    "jupyter_utils.make_submit_file(cmds=cmds, env=env, submitfile=cmds+'.sh', group_size=10, queue='cpu', timeout='24:00:00',cpus=1, mem='1G')\n",
    "p = subprocess.Popen(['sbatch', cmds+'.sh'])\n",
    "n_cmds = sum([1 for l in open(cmds,'r')])\n",
    "print(f'Prepared and submitted {n_cmds} jobs for clustering in {cmds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates dataframe with PWM and useless interface scores\n",
    "pwm_score_fs = glob.glob(f'{af2_path}/filtered_pdbs/pwm_results/*/specific_pos_count.txt')\n",
    "names = []\n",
    "pwm_scores = []\n",
    "on_target_pwm_scores = []\n",
    "for pwm_score_f in pwm_score_fs:\n",
    "    names.append(pwm_score_f.split('/')[-2])\n",
    "    with open(pwm_score_f,'r') as f_in:\n",
    "        lines = f_in.readlines()\n",
    "        pwm_scores.append(int(lines[0].split(' ')[0]))\n",
    "        on_target_pwm_scores.append(int(lines[0].split(' ')[1]))\n",
    "pwm_score_dict = {'out_tag':names,'pwm_score':pwm_scores,'on_target_pwm_score':on_target_pwm_scores}\n",
    "df_pwm_score = pd.DataFrame(pwm_score_dict)\n",
    "\n",
    "df_designs = pd.read_csv(f'{af2_path}/combined_filtered.csv')\n",
    "df_pwm_score = df_pwm_score.merge(df_designs, on='out_tag',how='inner')\n",
    "print(len(df_pwm_score))\n",
    "df_pwm_score.to_csv(f'{af2_path}/df_pwm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell filters the resulting outputs. You may play with filters to get a desired number of filtered outputs that look reasonable to you.\n",
    "# You may change fraction not passing to allow some number of designs that are below the filter thresholds. \n",
    "# This is useful for testing if filters are helping in experiments.\n",
    "# Use seqid_cut to cluster by sequence identify. seqid_cut of 1 will not cluster by sequence. \n",
    "\n",
    "fraction_not_passing = 0\n",
    "seqid_cut = 1\n",
    "\n",
    "df = pd.read_csv(f'{af2_path}/df_pwm.csv')\n",
    "\n",
    "threshold_dict = {'tot_on_tgt':[3.5,'>='],'on_target_pwm_score':[1,'>='],'base_score':[10,'>='],'phosphate_score':[0, '>='],'bidentate_score':[0, '>='],\n",
    "                  'max_rboltz_RKQE':[0.1, '>='],'avg_top_two_rboltz':[0.1, '>='],'n_backbone_phosphate_contacts':[0,'>='],\n",
    "                  'ddg':[-15,'<='],'contact_molecular_surface':[0,'>='],'net_charge_over_sasa':[-10,'>='],'ca_rmsd':[2,'<='],'plddt':[80,'>=']\n",
    "                    }\n",
    "\n",
    "df_filtered = useful_utils.filter_designs(mpnn2_path,df,threshold_dict,fraction_not_passing,seqid_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SILENT_FILE = f'{af2_path}/combined_filtered.silent'\n",
    "\n",
    "# These are your designs!!!\n",
    "selected_designs = []\n",
    "for j in range(len(df_filtered)):\n",
    "    selected_designs.append(str(df_filtered['out_tag'].iloc[j]))\n",
    "print(len(selected_designs))\n",
    "\n",
    "\n",
    "with open(f'{af2_path}/pwm_filtered_designs.list', 'w') as f:\n",
    "    for line in selected_designs:\n",
    "        f.write(line + '\\n')\n",
    "        \n",
    "print('Run this in the terminal to get your filtered silent file:\\n')\n",
    "print(f\"\"\"\n",
    "cat pwm_filtered_designs.list | silentslice {SILENT_FILE} > combined_filtered_pwm.silent\"\"\")\n",
    "\n",
    "df_filtered.to_csv(f'{af2_path}/combined_pwm_filtered.csv', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
